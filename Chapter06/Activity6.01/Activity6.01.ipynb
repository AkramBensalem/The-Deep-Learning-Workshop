{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 6.01: Sentiment Analysis of Amazon Product Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "#Read the data files for train and test set- Ensure that they are in the same folder where you're running the Jupyter notebook\n",
    "train_df = pd.read_csv(\"../Datasets/Activity6.01/Amazon_reviews_train.csv\")\n",
    "test_df = pd.read_csv(\"../Datasets/Activity6.01/Amazon_reviews_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2) (25000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Stuning even for the non-gamer: This sound tra...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>The best soundtrack ever to anything.: I'm rea...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Amazing!: This soundtrack is my favorite music...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Excellent Soundtrack: I truly like this soundt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Remember, Pull Your Jaw Off The Floor After He...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         review_text  label\n",
       "0  Stuning even for the non-gamer: This sound tra...      1\n",
       "1  The best soundtrack ever to anything.: I'm rea...      1\n",
       "2  Amazing!: This soundtrack is my favorite music...      1\n",
       "3  Excellent Soundtrack: I truly like this soundt...      1\n",
       "4  Remember, Pull Your Jaw Off The Floor After He...      1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print and examine first 5 records\n",
    "print(train_df.shape, train_df.shape)\n",
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For convenience in processing, separate the raw text and the labels for the train and test set.  Print the first two reviews from the train text.\n",
    "\n",
    "You should have 4 variables - \n",
    "\n",
    "  - train_raw: raw text for the train data\n",
    "  - train_labels: labels for the train data\n",
    "  - test_raw: raw text for the test data\n",
    "  - test_labels: labels for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = train_df.review_text.values\n",
    "train_labels = train_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_raw = test_df.review_text.values\n",
    "test_labels = test_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Stuning even for the non-gamer: This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^',\n",
       "       \"The best soundtrack ever to anything.: I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.5\n",
      "  Downloading nltk-3.5.zip (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.1 MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk==3.5) (0.14.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2020.6.8-cp37-cp37m-manylinux2010_x86_64.whl (661 kB)\n",
      "\u001b[K     |████████████████████████████████| 661 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk==3.5) (4.43.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=d5b263ec95a39c9dc6aa2bde69be3ccae7592d1cf4d09bf2955b0ffa1844478f\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\n",
      "Successfully built nltk\n",
      "Installing collected packages: click, regex, nltk\n",
      "Successfully installed click-7.1.2 nltk-3.5 regex-2020.6.8\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens = [word_tokenize(review.lower()) for review in train_raw]\n",
    "test_tokens = [word_tokenize(review.lower()) for review in test_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuning', 'even', 'for', 'the', 'non-gamer', ':', 'this', 'sound', 'track', 'was', 'beautiful', '!', 'it', 'paints', 'the', 'senery', 'in', 'your', 'mind', 'so', 'well', 'i', 'would', 'recomend', 'it', 'even', 'to', 'people', 'who', 'hate', 'vid', '.', 'game', 'music', '!', 'i', 'have', 'played', 'the', 'game', 'chrono', 'cross', 'but', 'out', 'of', 'all', 'of', 'the', 'games', 'i', 'have', 'ever', 'played', 'it', 'has', 'the', 'best', 'music', '!', 'it', 'backs', 'away', 'from', 'crude', 'keyboarding', 'and', 'takes', 'a', 'fresher', 'step', 'with', 'grate', 'guitars', 'and', 'soulful', 'orchestras', '.', 'it', 'would', 'impress', 'anyone', 'who', 'cares', 'to', 'listen', '!', '^_^']\n"
     ]
    }
   ],
   "source": [
    "#Print the first review from the train data to check if the tokenization worked.\n",
    "print(train_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "stop_punct = list(punctuation)\n",
    "#download the stopwords corpus \n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "stop_nltk = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_final = stop_punct + stop_nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_stop(input_tokens):\n",
    "    return [token for token in input_tokens if token not in stop_final]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_no_stop = [drop_stop(sent) for sent in train_tokens]\n",
    "test_tokens_no_stop = [drop_stop(sent) for sent in test_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stuning', 'even', 'non-gamer', 'sound', 'track', 'beautiful', 'paints', 'senery', 'mind', 'well', 'would', 'recomend', 'even', 'people', 'hate', 'vid', 'game', 'music', 'played', 'game', 'chrono', 'cross', 'games', 'ever', 'played', 'best', 'music', 'backs', 'away', 'crude', 'keyboarding', 'takes', 'fresher', 'step', 'grate', 'guitars', 'soulful', 'orchestras', 'would', 'impress', 'anyone', 'cares', 'listen', '^_^']\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens_no_stop[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer_p = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens_stem = [[stemmer_p.stem(token) for token in sent] for sent in train_tokens_no_stop]\n",
    "test_tokens_stem = [[stemmer_p.stem(token) for token in sent] for sent in test_tokens_no_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stune', 'even', 'non-gam', 'sound', 'track', 'beauti', 'paint', 'seneri', 'mind', 'well', 'would', 'recomend', 'even', 'peopl', 'hate', 'vid', 'game', 'music', 'play', 'game', 'chrono', 'cross', 'game', 'ever', 'play', 'best', 'music', 'back', 'away', 'crude', 'keyboard', 'take', 'fresher', 'step', 'grate', 'guitar', 'soul', 'orchestra', 'would', 'impress', 'anyon', 'care', 'listen', '^_^']\n"
     ]
    }
   ],
   "source": [
    "print(train_tokens_stem[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [\" \".join(txt) for txt in train_tokens_stem]\n",
    "test_texts = [\" \".join(txt) for txt in test_tokens_stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stune even non-gam sound track beauti paint seneri mind well would recomend even peopl hate vid game music play game chrono cross game ever play best music back away crude keyboard take fresher step grate guitar soul orchestra would impress anyon care listen ^_^\n"
     ]
    }
   ],
   "source": [
    "print(train_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(num_words=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.fit_on_texts(train_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tok.texts_to_sequences(train_texts)\n",
    "test_sequences = tok.texts_to_sequences(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22, 516, 7595, 85, 190, 184, 1097, 282, 20, 11, 1265, 22, 56, 370, 9662, 114, 41, 71, 114, 8158, 1454, 114, 51, 71, 29, 41, 58, 182, 2929, 2151, 76, 8159, 816, 2663, 829, 718, 3869, 11, 483, 120, 268, 109]\n"
     ]
    }
   ],
   "source": [
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lens = [len(seq) for seq in train_sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(seq_lens)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(train_sequences, maxlen=maxlen)\n",
    "X_test = pad_sequences(test_sequences, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 100)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, SpatialDropout1D, Dropout, GRU, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.add(Embedding(vocab_size, output_dim=32))\n",
    "model_lstm.add(SpatialDropout1D(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.add(LSTM(64, return_sequences=True))\n",
    "model_lstm.add(LSTM(64, return_sequences=False))\n",
    "model_lstm.add(Dropout(0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.add(Dense(32, activation='relu'))\n",
    "model_lstm.add(Dropout(0.5))\n",
    "model_lstm.add(Dense(32, activation='relu'))\n",
    "model_lstm.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          320000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, None, 32)          0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 64)          24832     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 381,025\n",
      "Trainable params: 381,025\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_lstm.add(Dense(1, activation='sigmoid'))\n",
    " \n",
    "model_lstm.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 166s 8ms/sample - loss: 0.5846 - accuracy: 0.6787 - val_loss: 0.4463 - val_accuracy: 0.8048\n",
      "Epoch 2/5\n",
      " 2688/20000 [===>..........................] - ETA: 2:06 - loss: 0.4144 - accuracy: 0.8415"
     ]
    }
   ],
   "source": [
    "history_lstm = model_lstm.fit(X_train, train_labels, batch_size=128, validation_split=0.2, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model_lstm.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(test_labels, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(test_labels, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
