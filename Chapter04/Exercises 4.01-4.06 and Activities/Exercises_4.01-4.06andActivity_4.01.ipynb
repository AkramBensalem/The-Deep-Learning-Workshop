{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"pip install nltk==3.5.0","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting nltk==3.5.0\n  Downloading nltk-3.5.zip (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 4.2 MB/s eta 0:00:01\n\u001b[?25hCollecting click\n  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n\u001b[K     |████████████████████████████████| 82 kB 951 kB/s  eta 0:00:01\n\u001b[?25hRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk==3.5.0) (0.14.1)\nCollecting regex\n  Downloading regex-2020.6.8-cp37-cp37m-manylinux2010_x86_64.whl (661 kB)\n\u001b[K     |████████████████████████████████| 661 kB 16.4 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: tqdm in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk==3.5.0) (4.43.0)\nBuilding wheels for collected packages: nltk\n  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434678 sha256=ef290002fa6899d6530f5e8e96f8ae10a19d123e5b8a71b3f46b49a088885e18\n  Stored in directory: /home/jovyan/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\nSuccessfully built nltk\nInstalling collected packages: click, regex, nltk\nSuccessfully installed click-7.1.2 nltk-3.5 regex-2020.6.8\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pip install gensim","execution_count":2,"outputs":[{"output_type":"stream","text":"Collecting gensim\n  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n\u001b[K     |████████████████████████████████| 24.2 MB 4.5 MB/s eta 0:00:01    |████████▏                       | 6.2 MB 4.5 MB/s eta 0:00:05\n\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from gensim) (1.4.1)\nCollecting smart-open>=1.8.1\n  Downloading smart_open-2.1.0.tar.gz (116 kB)\n\u001b[K     |████████████████████████████████| 116 kB 22.3 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: six>=1.5.0 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from gensim) (1.14.0)\nRequirement already satisfied: numpy>=1.11.3 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from gensim) (1.18.1)\nRequirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.7/site-packages (from smart-open>=1.8.1->gensim) (2.23.0)\nCollecting boto\n  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 28.8 MB/s eta 0:00:01\n\u001b[?25hCollecting boto3\n  Downloading boto3-1.14.19-py2.py3-none-any.whl (128 kB)\n\u001b[K     |████████████████████████████████| 128 kB 27.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.11.28)\nRequirement already satisfied: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (2.9)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->smart-open>=1.8.1->gensim) (1.25.8)\nCollecting jmespath<1.0.0,>=0.7.1\n  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\nCollecting s3transfer<0.4.0,>=0.3.0\n  Downloading s3transfer-0.3.3-py2.py3-none-any.whl (69 kB)\n\u001b[K     |████████████████████████████████| 69 kB 11.2 MB/s eta 0:00:01\n\u001b[?25hCollecting botocore<1.18.0,>=1.17.19\n  Downloading botocore-1.17.19-py2.py3-none-any.whl (6.3 MB)\n\u001b[K     |████████████████████████████████| 6.3 MB 30.8 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from botocore<1.18.0,>=1.17.19->boto3->smart-open>=1.8.1->gensim) (2.8.1)\nCollecting docutils<0.16,>=0.10\n  Downloading docutils-0.15.2-py3-none-any.whl (547 kB)\n\u001b[K     |████████████████████████████████| 547 kB 30.3 MB/s eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: smart-open\n  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110317 sha256=e3988d0cffa61b68b6daca35cc3125b78bbd4673c584113911f736d8a69e7a1b\n  Stored in directory: /home/jovyan/.cache/pip/wheels/56/b5/6d/86dbe4f29d4688e5163a8b8c6b740494310040286fca4dc648\nSuccessfully built smart-open\nInstalling collected packages: boto, jmespath, docutils, botocore, s3transfer, boto3, smart-open, gensim\nSuccessfully installed boto-2.49.0 boto3-1.14.19 botocore-1.17.19 docutils-0.15.2 gensim-3.8.3 jmespath-0.10.0 s3transfer-0.3.3 smart-open-2.1.0\nNote: you may need to restart the kernel to use updated packages.\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container {width:80% !important;}</style>\"))","execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>.container {width:80% !important;}</style>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_txt = \"\"\"Welcome to the world of Deep Learning for NLP! We're in this together, and we'll learn together. \nNLP is amazing, and Deep Learning makes it even more fun. Let's learn!\"\"\"","execution_count":4,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('punkt')\nfrom nltk import tokenize","execution_count":5,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenize.sent_tokenize(raw_txt)","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"['Welcome to the world of Deep Learning for NLP!',\n \"We're in this together, and we'll learn together.\",\n 'NLP is amazing, and Deep Learning makes it even more fun.',\n \"Let's learn!\"]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_sents = tokenize.sent_tokenize(raw_txt)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(txt_sents), len(txt_sents)","execution_count":8,"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"(list, 4)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]\ntype(txt_words), type(txt_words[0])","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(list, list)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(txt_words[:2])","execution_count":10,"outputs":[{"output_type":"stream","text":"[['Welcome', 'to', 'the', 'world', 'of', 'Deep', 'Learning', 'for', 'NLP', '!'], ['We', \"'re\", 'in', 'this', 'together', ',', 'and', 'we', \"'ll\", 'learn', 'together', '.']]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Normalizing case"},{"metadata":{"trusted":true},"cell_type":"code","source":"#You needn't run this\nraw_txt = raw_txt.lower()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_sents = [sent.lower() for sent in txt_sents]\ntxt_sents","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"['welcome to the world of deep learning for nlp!',\n \"we're in this together, and we'll learn together.\",\n 'nlp is amazing, and deep learning makes it even more fun.',\n \"let's learn!\"]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(txt_words[:2])","execution_count":14,"outputs":[{"output_type":"stream","text":"[['welcome', 'to', 'the', 'world', 'of', 'deep', 'learning', 'for', 'nlp', '!'], ['we', \"'re\", 'in', 'this', 'together', ',', 'and', 'we', \"'ll\", 'learn', 'together', '.']]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Removing punctuation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list_punct = list(punctuation)\nprint(list_punct)","execution_count":16,"outputs":[{"output_type":"stream","text":"['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_punct(input_tokens):\n    return [token for token in input_tokens if token not in list_punct]","execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"drop_punct([\"let\",\".\",\"us\",\".\",\"go\",\"!\"])","execution_count":18,"outputs":[{"output_type":"execute_result","execution_count":18,"data":{"text/plain":"['let', 'us', 'go']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_words_nopunct = [drop_punct(sent) for sent in txt_words]\nprint(txt_words_nopunct)","execution_count":19,"outputs":[{"output_type":"stream","text":"[['welcome', 'to', 'the', 'world', 'of', 'deep', 'learning', 'for', 'nlp'], ['we', \"'re\", 'in', 'this', 'together', 'and', 'we', \"'ll\", 'learn', 'together'], ['nlp', 'is', 'amazing', 'and', 'deep', 'learning', 'makes', 'it', 'even', 'more', 'fun'], ['let', \"'s\", 'learn']]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### Removing stop words"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download(\"stopwords\")","execution_count":20,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n","name":"stderr"},{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import stopwords\nlist_stop = stopwords.words(\"english\")\nlen(list_stop)","execution_count":21,"outputs":[{"output_type":"execute_result","execution_count":21,"data":{"text/plain":"179"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(list_stop[:50])","execution_count":22,"outputs":[{"output_type":"stream","text":"['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Exercise 4.01: Tokenizing, Case Normalization, Punctuation and Stop Word Removal"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk import tokenize","execution_count":23,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"raw_txt = \"\"\"Welcome to the world of Deep Learning for NLP! We're in this together, and we'll learn together. NLP is amazing, and Deep Learning makes it even more fun. Let's learn!\"\"\"","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_sents = tokenize.sent_tokenize(raw_txt.lower())\ntxt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]\n\nfrom string import punctuation\nstop_punct = list(punctuation)\n\nfrom nltk.corpus import stopwords\nstop_nltk = stopwords.words(\"english\")\n\nstop_final = stop_punct + stop_nltk","execution_count":25,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_stop(input_tokens):\n    return [token for token in input_tokens if token not in stop_final]","execution_count":26,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_words_nostop = [drop_stop(sent) for sent in txt_words]","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(txt_words_nostop[0])","execution_count":28,"outputs":[{"output_type":"stream","text":"['welcome', 'world', 'deep', 'learning', 'nlp']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Stemming"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer","execution_count":29,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer_p = PorterStemmer()","execution_count":30,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(stemmer_p.stem(\"driving\"))","execution_count":31,"outputs":[{"output_type":"stream","text":"drive\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt = \"I mustered all my drive, drove to the driving school!\"","execution_count":32,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokens = tokenize.word_tokenize(txt)\nprint([stemmer_p.stem(word) for word in tokens])","execution_count":33,"outputs":[{"output_type":"stream","text":"['I', 'muster', 'all', 'my', 'drive', ',', 'drove', 'to', 'the', 'drive', 'school', '!']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Lemmatization"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('wordnet')","execution_count":34,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/wordnet.zip.\n","name":"stderr"},{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer \nlemmatizer = WordNetLemmatizer()","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lemmatizer.lemmatize(\"ponies\")","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"'pony'"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Exercise 4.02: Stemming Our Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer","execution_count":37,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer_p = PorterStemmer()","execution_count":38,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print([stemmer_p.stem(token) for token in txt_words_nostop[0]])","execution_count":39,"outputs":[{"output_type":"stream","text":"['welcom', 'world', 'deep', 'learn', 'nlp']\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Applying stemmer to all the sentences"},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_words_stem = [[stemmer_p.stem(token) for token in sent] for sent in txt_words_nostop]","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_words_stem","execution_count":41,"outputs":[{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"[['welcom', 'world', 'deep', 'learn', 'nlp'],\n [\"'re\", 'togeth', \"'ll\", 'learn', 'togeth'],\n ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],\n ['let', \"'s\", 'learn']]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Downloading Text Corpora using NLTK"},{"metadata":{"trusted":true},"cell_type":"code","source":"import nltk\nnltk.download('gutenberg')","execution_count":42,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package gutenberg to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/gutenberg.zip.\n","name":"stderr"},{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download()","execution_count":43,"outputs":[{"output_type":"stream","text":"NLTK Downloader\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\nDownloader> q\n","name":"stdout"},{"output_type":"execute_result","execution_count":43,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"alice_raw = nltk.corpus.gutenberg.raw('carroll-alice.txt')","execution_count":44,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"alice_raw[:800]","execution_count":45,"outputs":[{"output_type":"execute_result","execution_count":45,"data":{"text/plain":"\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit\""},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Representation\n### 1. One hot encoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_words_nostop","execution_count":46,"outputs":[{"output_type":"execute_result","execution_count":46,"data":{"text/plain":"[['welcome', 'world', 'deep', 'learning', 'nlp'],\n [\"'re\", 'together', \"'ll\", 'learn', 'together'],\n ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],\n ['let', \"'s\", 'learn']]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Exercise 4.03: Creating One-Hot Encoding for Our Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"print(txt_words_nostop)","execution_count":47,"outputs":[{"output_type":"stream","text":"[['welcome', 'world', 'deep', 'learning', 'nlp'], [\"'re\", 'together', \"'ll\", 'learn', 'together'], ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'], ['let', \"'s\", 'learn']]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"target_terms = [\"nlp\",\"deep\",\"learn\"]","execution_count":48,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_onehot(sent):\n    return [1 if term in  sent else 0 for term in target_terms]","execution_count":49,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"one_hot_mat = [get_onehot(sent) for sent in txt_words_nostop]","execution_count":50,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np","execution_count":51,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"np.array(one_hot_mat)","execution_count":52,"outputs":[{"output_type":"execute_result","execution_count":52,"data":{"text/plain":"array([[1, 1, 0],\n       [0, 0, 1],\n       [1, 1, 0],\n       [0, 0, 1]])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Term Frequencies"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer","execution_count":53,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(max_features = 5)","execution_count":54,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.fit(txt_sents)","execution_count":55,"outputs":[{"output_type":"execute_result","execution_count":55,"data":{"text/plain":"CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=5, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.vocabulary_","execution_count":56,"outputs":[{"output_type":"execute_result","execution_count":56,"data":{"text/plain":"{'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_dtm = vectorizer.fit_transform(txt_sents)","execution_count":57,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_dtm.toarray()","execution_count":58,"outputs":[{"output_type":"execute_result","execution_count":58,"data":{"text/plain":"array([[0, 1, 0, 0, 0],\n       [1, 0, 1, 2, 2],\n       [1, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_sents","execution_count":59,"outputs":[{"output_type":"execute_result","execution_count":59,"data":{"text/plain":"['welcome to the world of deep learning for nlp!',\n \"we're in this together, and we'll learn together.\",\n 'nlp is amazing, and deep learning makes it even more fun.',\n \"let's learn!\"]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def do_nothing(doc):\n    return doc","execution_count":60,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer = CountVectorizer(max_features=5, \n                             preprocessor=do_nothing, \n                             tokenizer=do_nothing)","execution_count":61,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_dtm = vectorizer.fit_transform(txt_words_stem)","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_dtm.toarray()","execution_count":63,"outputs":[{"output_type":"execute_result","execution_count":63,"data":{"text/plain":"array([[0, 1, 1, 1, 0],\n       [1, 0, 1, 0, 2],\n       [0, 1, 1, 1, 0],\n       [0, 0, 1, 0, 0]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.vocabulary_","execution_count":64,"outputs":[{"output_type":"execute_result","execution_count":64,"data":{"text/plain":"{'deep': 1, 'learn': 2, 'nlp': 3, 'togeth': 4, \"'ll\": 0}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_words_stem","execution_count":65,"outputs":[{"output_type":"execute_result","execution_count":65,"data":{"text/plain":"[['welcom', 'world', 'deep', 'learn', 'nlp'],\n [\"'re\", 'togeth', \"'ll\", 'learn', 'togeth'],\n ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],\n ['let', \"'s\", 'learn']]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Exercise 4.04: Document Term Matrix with TF-IDF"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","execution_count":66,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_tfidf = TfidfVectorizer(max_features=5)","execution_count":67,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_tfidf.fit(txt_sents)","execution_count":68,"outputs":[{"output_type":"execute_result","execution_count":68,"data":{"text/plain":"TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.float64'>, encoding='utf-8',\n                input='content', lowercase=True, max_df=1.0, max_features=5,\n                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n                smooth_idf=True, stop_words=None, strip_accents=None,\n                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, use_idf=True, vocabulary=None)"},"metadata":{}}]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"vectorizer_tfidf.vocabulary_","execution_count":69,"outputs":[{"output_type":"execute_result","execution_count":69,"data":{"text/plain":"{'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_tfidf = vectorizer_tfidf.transform(txt_sents)","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_tfidf.toarray()","execution_count":71,"outputs":[{"output_type":"execute_result","execution_count":71,"data":{"text/plain":"array([[0.        , 1.        , 0.        , 0.        , 0.        ],\n       [0.25932364, 0.        , 0.25932364, 0.65783832, 0.65783832],\n       [0.70710678, 0.70710678, 0.        , 0.        , 0.        ],\n       [0.        , 0.        , 1.        , 0.        , 0.        ]])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer_tfidf.idf_","execution_count":72,"outputs":[{"output_type":"execute_result","execution_count":72,"data":{"text/plain":"array([1.51082562, 1.51082562, 1.51082562, 1.91629073, 1.91629073])"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Training Our Own Embeddings"},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"import gensim.downloader as api\nfrom gensim.models import word2vec","execution_count":73,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Another way of loading the data. if this doesn't work, you could use the text8 corpus local file\ndataset = api.load(\"text8\")","execution_count":74,"outputs":[{"output_type":"stream","text":"[==================================================] 100.0% 31.6/31.6MB downloaded\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"To ensure reproducible results, set random seed as 1"},{"metadata":{"trusted":true},"cell_type":"code","source":"np.random.seed(1)","execution_count":75,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = word2vec.Text8Corpus(\"text8\")","execution_count":76,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model = word2vec.Word2Vec(dataset)","execution_count":77,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"print(model.wv[\"animal\"])","execution_count":78,"outputs":[{"output_type":"stream","text":"[-8.5838413e-01  1.4214208e+00  4.6447900e-01  7.0885710e-02\n  1.6896889e-01 -9.8778456e-02  1.1402733e+00 -2.9789758e+00\n  2.8246179e-01 -8.7568551e-02 -1.2357916e+00 -1.2885849e-01\n  1.2703731e+00 -1.4592729e-02 -3.8777024e-01  2.9091379e-01\n  9.5472641e-02  1.4563963e-01 -6.8076456e-01 -9.9715537e-01\n  1.4350164e+00  2.1312969e+00  1.9116565e-03  1.0840468e+00\n  7.8456789e-01  1.2306607e+00 -4.5679808e-01 -2.3956151e+00\n  4.6197531e-01  2.9375532e-01 -4.0576980e-01  2.0033605e+00\n -1.4525657e+00 -4.6804035e-01 -9.8208651e-02  8.8128763e-01\n  9.9858916e-01  2.4697350e-01  1.6031005e+00 -1.2744416e+00\n  7.7264303e-01 -1.7759993e+00 -3.0845227e+00 -1.5518727e+00\n -6.5049249e-01  7.8414708e-01 -1.0474778e+00  3.9749783e-01\n -2.6754682e+00 -4.4862261e-01 -1.3782256e+00  1.5887188e+00\n  1.4766140e+00  4.3363068e-01 -2.2527428e-01 -2.4994092e-01\n  1.0335336e+00 -5.8475828e-01 -1.4084058e+00  8.6268568e-01\n -1.6042060e+00  1.5416752e+00 -2.1892607e+00 -2.9363409e-01\n -1.3044649e+00 -1.3346639e+00  2.6779032e+00 -9.6029192e-01\n -2.0819674e-01  8.6686528e-01 -4.0506777e-01 -6.0530275e-01\n  8.5680288e-01 -2.2170002e+00 -7.9418045e-01  3.8755706e-01\n -1.2936714e+00  1.9454813e+00 -3.3980820e+00  8.3567783e-02\n -1.5696157e+00  9.1862988e-01  2.1667085e+00  5.0387460e-01\n -5.2025598e-01  2.0608380e+00 -8.0719841e-01  4.7570226e-01\n -1.0792695e+00  1.1881932e+00 -1.3966294e-01 -3.4716696e-02\n -1.7193992e-01 -2.3347569e+00  9.6631718e-01  5.7336640e-01\n  9.7014022e-01  7.4097663e-01  1.4732680e+00  7.8760952e-01]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(model.wv[\"animal\"])","execution_count":79,"outputs":[{"output_type":"execute_result","execution_count":79,"data":{"text/plain":"100"},"metadata":{}}]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"model.wv.most_similar(\"animal\")","execution_count":80,"outputs":[{"output_type":"execute_result","execution_count":80,"data":{"text/plain":"[('insect', 0.7552701830863953),\n ('animals', 0.7310898303985596),\n ('feces', 0.6877481937408447),\n ('aquatic', 0.6783003807067871),\n ('sentient', 0.6687874794006348),\n ('mammal', 0.6686221361160278),\n ('insects', 0.6608380675315857),\n ('eating', 0.6508541107177734),\n ('humans', 0.6505103707313538),\n ('ants', 0.6468088626861572)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar(\"happiness\")","execution_count":81,"outputs":[{"output_type":"execute_result","execution_count":81,"data":{"text/plain":"[('goodness', 0.7682030200958252),\n ('compassion', 0.7660053968429565),\n ('humanity', 0.7659634351730347),\n ('perfection', 0.746347963809967),\n ('pleasure', 0.7299338579177856),\n ('mankind', 0.7249332666397095),\n ('desires', 0.7136735916137695),\n ('conscious', 0.7103390693664551),\n ('immortality', 0.7067128419876099),\n ('dignity', 0.7043787837028503)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Semantic Regularities in Word Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)","execution_count":82,"outputs":[{"output_type":"execute_result","execution_count":82,"data":{"text/plain":"[('queen', 0.6854889392852783),\n ('elizabeth', 0.6392526030540466),\n ('prince', 0.6301933526992798),\n ('empress', 0.6284343004226685),\n ('princess', 0.6062546968460083)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar(positive=['uncle', 'woman'], negative=['man'], topn=5)","execution_count":83,"outputs":[{"output_type":"execute_result","execution_count":83,"data":{"text/plain":"[('aunt', 0.8330071568489075),\n ('grandmother', 0.8115813732147217),\n ('wife', 0.8074036240577698),\n ('niece', 0.8025387525558472),\n ('edith', 0.7721768617630005)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Exercise 4.05: Vectors for Phrases"},{"metadata":{"trusted":true},"cell_type":"code","source":"v1 = model.wv['get']\nv2 = model.wv['happy']\nres1 = (v1+v2)/2","execution_count":84,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"v1 = model.wv['make']\nv2 = model.wv['merry']\nres2 = (v1+v2)/2","execution_count":85,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.cosine_similarities(res1, [res2])","execution_count":86,"outputs":[{"output_type":"execute_result","execution_count":86,"data":{"text/plain":"array([0.5505672], dtype=float32)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Effect of Parameters - 'size' of the Vector"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = word2vec.Word2Vec(dataset, size=30)","execution_count":87,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=5)","execution_count":88,"outputs":[{"output_type":"execute_result","execution_count":88,"data":{"text/plain":"[('empress', 0.8615267276763916),\n ('emperor', 0.8606838583946228),\n ('prince', 0.8221743106842041),\n ('regent', 0.820101261138916),\n ('son', 0.8148695826530457)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Effect of parameters - skipgram vs. CBOW"},{"metadata":{},"cell_type":"markdown","source":"#### Rare terms - oeuvre"},{"metadata":{"trusted":true},"cell_type":"code","source":"model = word2vec.Word2Vec(dataset)","execution_count":89,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.wv.most_similar(\"oeuvre\", topn=5)","execution_count":90,"outputs":[{"output_type":"execute_result","execution_count":90,"data":{"text/plain":"[('surrealist', 0.7391599416732788),\n ('orchestration', 0.7302736043930054),\n ('lithographs', 0.720194935798645),\n ('sonnet', 0.7199317216873169),\n ('nieve', 0.7163490056991577)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sg = word2vec.Word2Vec(dataset, sg=1)","execution_count":91,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_sg.wv.most_similar(\"oeuvre\", topn=5)","execution_count":92,"outputs":[{"output_type":"execute_result","execution_count":92,"data":{"text/plain":"[('masterful', 0.8615211248397827),\n ('lithographs', 0.8272578120231628),\n ('impressionistic', 0.821493923664093),\n ('prolifically', 0.8097631931304932),\n ('speght', 0.8088525533676147)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Exercise 4.06: Training Word Vectors on Different Datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"nltk.download('brown')\nnltk.download('movie_reviews')","execution_count":93,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package brown to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/brown.zip.\n[nltk_data] Downloading package movie_reviews to\n[nltk_data]     /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/movie_reviews.zip.\n","name":"stderr"},{"output_type":"execute_result","execution_count":93,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.corpus import brown, movie_reviews","execution_count":94,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_brown = word2vec.Word2Vec(brown.sents(), sg=1)\nmodel_movie = word2vec.Word2Vec(movie_reviews.sents(), sg=1)","execution_count":95,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_brown.wv.most_similar('money', topn=5)","execution_count":96,"outputs":[{"output_type":"execute_result","execution_count":96,"data":{"text/plain":"[('care', 0.8496509194374084),\n ('friendship', 0.844196081161499),\n ('job', 0.8403249979019165),\n ('permission', 0.8254338502883911),\n ('anywhere', 0.824324369430542)]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_movie.wv.most_similar('money', topn=5)","execution_count":97,"outputs":[{"output_type":"execute_result","execution_count":97,"data":{"text/plain":"[('cash', 0.7355437278747559),\n ('record', 0.7169833183288574),\n ('ransom', 0.6896260976791382),\n ('bucks', 0.6831791400909424),\n ('pact', 0.6827759146690369)]"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### Using Pre-Trained Word Vectors"},{"metadata":{},"cell_type":"markdown","source":"### Activity 4.01: Text Preprocessing of the 'Alice in Wonderland' Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"alice_raw[:800]","execution_count":98,"outputs":[{"output_type":"execute_result","execution_count":98,"data":{"text/plain":"\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning to get very tired of sitting by her sister on the\\nbank, and of having nothing to do: once or twice she had peeped into the\\nbook her sister was reading, but it had no pictures or conversations in\\nit, 'and what is the use of a book,' thought Alice 'without pictures or\\nconversation?'\\n\\nSo she was considering in her own mind (as well as she could, for the\\nhot day made her feel very sleepy and stupid), whether the pleasure\\nof making a daisy-chain would be worth the trouble of getting up and\\npicking the daisies, when suddenly a White Rabbit with pink eyes ran\\nclose by her.\\n\\nThere was nothing so VERY remarkable in that; nor did Alice think it so\\nVERY much out of the way to hear the Rabbit\""},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"#### Solution"},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_sents = tokenize.sent_tokenize(alice_raw.lower())","execution_count":99,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]","execution_count":100,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from string import punctuation\nstop_punct = list(punctuation)\n\nfrom nltk.corpus import stopwords\nstop_nltk = stopwords.words(\"english\")","execution_count":101,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_context = [\"--\", \"said\"]","execution_count":102,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"stop_final = stop_punct + stop_nltk + stop_context","execution_count":103,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def drop_stop(input_tokens):\n    return [token for token in input_tokens if token not in stop_final]","execution_count":104,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alice_words_nostop = [drop_stop(sent) for sent in txt_words]\nprint(alice_words_nostop[:2])","execution_count":105,"outputs":[{"output_type":"stream","text":"[['alice', \"'s\", 'adventures', 'wonderland', 'lewis', 'carroll', '1865', 'chapter', 'i.', 'rabbit-hole', 'alice', 'beginning', 'get', 'tired', 'sitting', 'sister', 'bank', 'nothing', 'twice', 'peeped', 'book', 'sister', 'reading', 'pictures', 'conversations', \"'and\", 'use', 'book', 'thought', 'alice', \"'without\", 'pictures', 'conversation'], ['considering', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepy', 'stupid', 'whether', 'pleasure', 'making', 'daisy-chain', 'would', 'worth', 'trouble', 'getting', 'picking', 'daisies', 'suddenly', 'white', 'rabbit', 'pink', 'eyes', 'ran', 'close']]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import PorterStemmer\nstemmer_p = PorterStemmer()","execution_count":106,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"alice_words_stem = [[stemmer_p.stem(token) for token in sent] for sent in alice_words_nostop]","execution_count":107,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(alice_words_stem[:5])","execution_count":108,"outputs":[{"output_type":"stream","text":"[['alic', \"'s\", 'adventur', 'wonderland', 'lewi', 'carrol', '1865', 'chapter', 'i.', 'rabbit-hol', 'alic', 'begin', 'get', 'tire', 'sit', 'sister', 'bank', 'noth', 'twice', 'peep', 'book', 'sister', 'read', 'pictur', 'convers', \"'and\", 'use', 'book', 'thought', 'alic', \"'without\", 'pictur', 'convers'], ['consid', 'mind', 'well', 'could', 'hot', 'day', 'made', 'feel', 'sleepi', 'stupid', 'whether', 'pleasur', 'make', 'daisy-chain', 'would', 'worth', 'troubl', 'get', 'pick', 'daisi', 'suddenli', 'white', 'rabbit', 'pink', 'eye', 'ran', 'close'], ['noth', 'remark', 'alic', 'think', 'much', 'way', 'hear', 'rabbit', 'say', \"'oh\", 'dear'], ['oh', 'dear'], ['shall', 'late']]\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":2}